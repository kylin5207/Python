# 模型参数自动化搜索

## 一、GridSearch

> 这个比较熟悉一些，机器学习模型经常使用它来找寻超参数。
>
> 超参数：在模型训练前设定～

### 1. 简述

​	GridSearch作为一种调参手段，其实采用的是“穷举搜索”的手段。在所有候选的参数中，通过循环遍历，尝试所有可能的参数组合，表现最好的参数对就是最终的结果。例如，使用高斯核的SVM模型的参数有两个gamma(核系数)和C(正则化系数)，参数gamma有3种可能的选择，参数C有4种可能的选择，那么通过表格的形式，可以得到12对参数值。GridSearch将遍历每个参数对，循环过程就像是在每个网格里搜索。

| gamma\C  | 0.1                   | 1                   | 10                   | 100                   |
| -------- | --------------------- | ------------------- | -------------------- | --------------------- |
| **0.01** | SVC(C=0.1, gamma=0.1) | SVC(C=1, gamma=0.1) | SVC(C=10, gamma=0.1) | SVC(C=100, gamma=0.1) |
| **0.1**  | SVC(C=0.1, gamma=1)   | SVC(C=1, gamma=1)   | SVC(C=10, gamma=1)   | SVC(C=100, gamma=1)   |
| **1**    | SVC(C=0.1, gamma=10)  | SVC(C=1, gamma=10)  | SVC(C=10, gamma=10)  | SVC(C=100, gamma=10)  |



### 2. 使用

#### (1) 如果手动搜索

```python
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 1. 加载数据
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

# 2. 手动暴力搜索
best_score = 0
for gamma in [0.01, 0.1, 1]:
    for c in [0.1, 1, 10, 100]:
        svm = SVC(C=c, gamma=gamma)
        svm.fit(X_train, y_train)
        score = svm.score(X_test, y_test)

        if score > best_score:
            best_score = score
            best_parameters = {"C":c, "gamma":gamma}

print(f"best parameters = {best_parameters}")
print(f"best score = {best_score}")
```

![image-20220729103848891](/Users/a123/Library/Application Support/typora-user-images/image-20220729103848891.png)

#### (2) 使用GridSearch

```python
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV

# 加载数据
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

# 定义网格参数
param_grid = {"C": [0.1, 1, 10, 100],
              "gamma": [0.01, 0.1, 1]}

# 利用网格参数和所需模型estimator，实例化GridSearch对象
grid_search = GridSearchCV(SVC(), param_grid, cv=5)

# 调用fit
grid_search.fit(X_train, y_train)
print("====best params===")
print(grid_search.best_params_)
print("===pred score===")
pred_score = grid_search.score(X_test, y_test)
print(pred_score)
```

![image-20220729105038458](/Users/a123/Library/Application Support/typora-user-images/image-20220729105038458.png)

- 拟合GridSearchCV对象不仅会搜索最佳参数，还会利用得到最佳交叉验证性能的参数在整个训练数据集上自动拟合一个新模型
- 使用best_params_属性可以查看最佳参数
- 而且最终得到的是一个最佳模型，包含predict，score等方法。

### 3. 总结

   由于该方法仅适用于机器学习模型的超参搜索，需要传入模型和拟合数据。我们只是为接口调试，选择一个合适的参数，无法提供数据集信息，所以不太适合。



## 二、Ray.tune

#### 1. 简述

​	Ray Tune是一个用来实验执行和超参数调优的Python包，其中集成了网格搜索、随机搜索、贝叶斯优化搜索（BayesOptSearch）等搜索算法以及Optuna, Hyperopt等优化工具。Ray Tune调参的模型可以是基于PyTorch, XGBoost, TensorFlow或Keras等框架构建的模型。

#### 2. 使用

​	(1) 需要先安装ray包

```python
pip install ray  
```

​	(2) 构建trainable

​	“Trainable”是一个需要在Tune.run()函数（见下文）运行时输入的参数，表示每一次的训练、调参及模型保存过程，可以用一个函数构建或用一个类（必须继承自tune.Trainable类）来构建。Trainable接受传入的参数config表示超参搜索空间，每次迭代tune.report()函数返回当前训练的结果，其余部分与正常的模型训练相同。

```python
def train_mnist(config):
    # 加载数据
    mnist_transforms = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.1307,), (0.3081,))])

    train_loader = DataLoader(
        datasets.MNIST("~/data", train=True, download=True, transform=mnist_transforms),
        batch_size=64,
        shuffle=True)
    test_loader = DataLoader(
        datasets.MNIST("~/data", train=False, transform=mnist_transforms),
        batch_size=64,
        shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 定义模型
    model = ConvNet()
    model.to(device)

    # 优化器
    optimizer = optim.SGD(
        model.parameters(), lr=config["lr"], momentum=config["momentum"])

    # 训练过程
    for i in range(10):
        train(model, optimizer, train_loader)
        acc = test(model, test_loader)

        # 返回当前step验证结果到Ray Tune, 以决定是否停止等后续操作.
        tune.report(mean_accuracy=acc)

        if i % 5 == 0:
            # 保存当前模型文件.
            torch.save(model.state_dict(), "./model.pth")
```

> tune.report() 
>
> 每训练一个epoch后，tune.report()函数会返回当前的实验结果，以确定调参方向或者是否提前终止实验。

​	(3) 超参数搜索

```python
# 定义超参搜索空间
search_space = {
    "lr": tune.sample_from(lambda spec: 10 ** (-10 * np.random.rand())),
    "momentum": tune.uniform(0.1, 0.9),
}
```

​	开始执行搜索过程，首先初始化ray，然后执行tune.run()函数即可~

```python
# 初始化，设置分配给ray的资源数目，默认会使用当前设备的所有资源
ray.init(num_cpus=4, num_gpus=4)

#开始执行搜索
analysis = tune.run(
    train_mnist,
    num_samples=20, # 不同的超参数试验次数
    scheduler=ASHAScheduler(metric="mean_accuracy", mode="max"),
    config=search_space,
)
```

​    实验结果以及各类配置参数等都可以通过analysis获取:

```python
best_trial = analysis.best_trial  # Get best trial
best_config = analysis.best_config  # Get best trial's hyperparameters
best_logdir = analysis.best_logdir  # Get best trial's logdir
best_checkpoint = analysis.best_checkpoint  # Get best trial's best checkpoint
best_result = analysis.best_result  # Get best trial's last results

# 实验结果输出
print("Best trial is:", best_trial)
print("Best config is:", best_config)
print("Best logdir is:", best_logdir)
print("Best checkpoint is:", best_checkpoint)
print("Best result is:", best_result)
```



## 三、Hpyerpot

​	hyperopt 是一个自动调参工具，与 sklearn 的 GridSearchCV 相比，hyperopt 具有更加完善的功能，且模型不必符合 sklearn 接口规范。

### 1. 安装

```python
pip install hyperopt
```

### 2. 使用

​	需要提供的内容：最小化的目标函数，搜索空间，搜索的算法（可选）

​	(1) 先定义一个目标函数

​		目标函数接收参数值，然后返回一个函数的损失值

```python
def objective(args):
    x, y = args
    return x**2 + y**2
```

> 例如，计算x^2+y^2

​	 (2) 定义参数的搜索空间

```python
from hyperopt import hp
space = [hp.uniform('x', 0, 1), hp.normal('y', 0, 1)]
```

>这里的hp生成参数空间的使用方法和numpy很类似
>
>hp.uniform(label,low,high) ：参数在low和high之间均匀分布。
>
>hp.randint(label,upper)：返回一个在[0,upper)前闭后开的区间内的随机整数。
>
>hp.normal(label, mean, std)：正态分布

​	(3) 定义搜索算法

​	algo用于指定搜索算法：

- 随机搜索(hyperopt.rand.suggest)
- 模拟退火(hyper opt.anneal.suggest)
- TPE算法(hyperopt.tpe.suggest, Tree-structured Parzen Estimator Approach)

​	(4) 最小化目标函数

```python
best = fmin(objective, space, algo=rand.suggest, max_evals=100)
```

​	就找到了最小值



## 四、手动实现

​	针对目前的情况，打算先手动实现一个。为了适配各种应用接口，打算将待调参的配置信息写入yaml文件，然后自动测试脚本读取配置文件的内容，进行参数的选择。

### 1. 配置文件yaml

```python
user_space_size:
  min: 10
  max: 1000
```

> - 同一级的字段要对齐
>
> - 使用缩进表示不同的层级，但不允许使用tab
>
> - 冒号后面加空格
>
> - 区分大小写

### 2. 读取yaml文件

