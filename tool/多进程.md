# Multiprocess多进程

## 一、multiprocessing模块

​	`multiprocessing`模块提供了一个`Process`类来代表一个进程对象，multiprocessing模块像线程一样管理进程，这个是multiprocessing的核心，它与threading很相似，对多核CPU的利用率会比threading好的多。

- process类的构造方法：

  ```python
  __init__(self, group=None, target=None, name=None, args=(), kwargs={})
  ```

  > 参数说明：
  >
  > **group**：进程所属组（基本不用） 
  > **target**：表示调用对象
  > **args**：表示调用对象的位置参数元组
  > **name**：别名 
  > **kwargs**：表示调用对象的字典

```python
import multiprocessing


def do(n):             # 参数n由args=(1,)传入
    name = multiprocessing.current_process().name        # 获取当前进程的名字
    print(f"{name} is starting")
    print(f"worker {n}")
    return


if __name__ == '__main__':
    numList = []
    for i in range(5):
        p = multiprocessing.Process(target=do, args=(i,))      # (i,)中加入","表示元祖
        numList.append(p)
        print(numList)
        p.start()                 # 用start()方法启动进程，执行do()方法
        p.join()                  # 等待子进程结束以后再继续往下运行，通常用于进程间的同步
        print("Process end.")
```

> 输出结果：
>
> ```sh
> [<Process name='Process-1' parent=40016 initial>]
> Process-1 is starting
> worker 0
> Process end.
> [<Process name='Process-1' pid=40026 parent=40016 stopped exitcode=0>, <Process name='Process-2' parent=40016 initial>]
> Process-2 is starting
> worker 1
> Process end.
> [<Process name='Process-1' pid=40026 parent=40016 stopped exitcode=0>, <Process name='Process-2' pid=40029 parent=40016 stopped exitcode=0>, <Process name='Process-3' parent=40016 initial>]
> Process-3 is starting
> worker 2
> Process end.
> ```
>
> 通过打印numList可以看出当前进程结束后，再开始下一个进程。



## 二、Pool类

​	为了避免频繁创建销毁进程所需要的资源，Pool类可以提供指定数量的进程供用户调用。当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。

### 1. apply_async

函数原型：apply_async(func[, args=()[, kwds={}[, callback=None]]])

​	与apply用法一致，但它是非阻塞的且支持结果返回后进行回调

```sh
import time
from multiprocessing import Pool
from concurrent.futures import ProcessPoolExecutor

def run(fn):
    # fn: 函数参数是数据列表的一个元素
    time.sleep(1)
    return fn * fn

if __name__ == "__main__":
    testFL = [1, 2, 3, 4, 5, 6]
    results = []
    print('======串行执行开始======')  # 顺序执行(也就是串行执行，单进程)
    s = time.time()
    for fn in testFL:
        results.append(run(fn))
    t1 = time.time()
    print(f"串行执行时间 = {t1 - s}s")
    print(f"results = {results}")

    print("======多进程开始======")  # 创建多个进程，并行执行
    results = []
    with Pool(3) as p:
        for fn in testFL:
            results.append(p.apply_async(run, (fn,)))
        p.close() # 关闭进程池，不再接受新的进程
        p.join() # 主进程阻塞等待子进程的退出
    t2 = time.time()
    print(f"并行执行时间 = {t2 - t1}s")
    for res in results:
        print(res.get())
```



## 三、Joblib类

​	joblib是python中提供一系列轻量级管道操作的工具; 

- 函数的透明磁盘缓存和延迟重新计算(记忆模式);

- 容易且简单的平行计算;

- 比 pickle更快的 序列化和反序列化 的功能;

  joblib经过优化,在大数据量时可以更快且强大,并对[numpy](https://so.csdn.net/so/search?q=numpy&spm=1001.2101.3001.7020)数组进行特别优化;

### 1. install

```sh
pip install joblib
```

### 2. Parallel

```sh
class joblib.Parallel(n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs', batch_size='auto', temp_folder=None, max_nbytes='1M', mmap_mode='r', prefer=None, require=None)
```

参数解析：

- n_jobs: 进程数，

  - -1:表示所有cpu核数都会使用,例如8核,假设每核可以运行2个进程,则共16个进程并发;
  - 1:表示不并发,用在debug时; 注意在用pycharm debug时,此值必须为1,否则报错,且要手动杀掉进程; macos系统flask项目中杀进程命令如下:   lsof -i:端口 | grep Python |cut -b 9-13 | xargs kill -9
  - 其他正整数 表示并发个数;

- backend:选择的后台并发类型(建议使用loky)

  - loky: 默认值,第一次程序运行时会创建 n_jobs个loky进程(每次并发时都是这些loky进程进行消费),要多耗费一些时间(1-2s左右),后续不会耗费; 每个loky进程的生命周期在最后一个请求进来后没有消费的300s后自动被杀死;
  - multiprocessing:多进程,基于进程池(multiprocessing.Pool)；在多并发时，如3个请求进来会产生 3*n_jobs个进程;所有每个请求进来时都会耗费时间在进程创建和销毁上,这个时间基本为0.2-0.3 s左右; 虽然基于进程池,但是每个进程的生命周期还是在请求开始到请求结束之间;
  - threading: 多线程; 不用多说; 因为GIL锁的原因,只能在 IO密集的任务中适用;

- batch_size: 一次分派给每个工作人员的原子任务数。该`'auto'`策略跟踪批处理完成所需的时间，并使用启发式动态调整批处理大小以将时间保持在半秒左右。

- timeout:设置并发时处理每个任务的超时时间

  如果任何任务花费的时间更长，则会引发 TimeOutError。仅在 n_jobs != 1 时应用。

- verbose:输出运行时的信息日志。详细级别：如果不为零，则显示进度消息

### 3. delayed()函数，它捕获传递给函数的参数。

The delayed function is a simple trick to be able to create a tuple (function, args, kwargs) with a function-call syntax.

- delayed函数是一个简单的技巧，可以使用函数调用语法创建元组 (function, args, kwargs)。

- delayed是一个装饰器，它接受一个函数及其参数，并将它们包装成一个对象，该对象可以放入一个列表中，并根据需要弹出。

```python
import time
from joblib import Parallel, delayed

def run(fn):
    # fn: 函数参数是数据列表的一个元素
    time.sleep(1)
    return fn * fn

if __name__ == "__main__":
    testFL = [1, 2, 3, 4, 5, 6]
    results = []
    print('======串行执行开始======')  # 顺序执行(也就是串行执行，单进程)
    s = time.time()
    for fn in testFL:
        results.append(run(fn))
    t1 = time.time()
    print(f"串行执行时间 = {t1 - s}s")
    print(f"results = {results}")

    print("======多进程开始======")  # 创建多个进程，并行执行
    # 创建parallel_obj对象
    parallel_obj = Parallel(n_jobs=-1, verbose=100, backend='loky', timeout=10)
    # 开始调用被并行计算的函数,并给出结果; 实现方式为 调用内置的 __call__方法
    out = parallel_obj(delayed(run)(i) for i in testFL)
    t2 = time.time()
    print(f"并行执行时间 = {t2 - t1}s")
    print(out)
```

​	输出：

```sh
======串行执行开始======
串行执行时间 = 6.010523319244385s
results = [1, 4, 9, 16, 25, 36]
======多进程开始======
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.2s
[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:    2.2s remaining:    4.5s
[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.2s remaining:    2.2s
[Parallel(n_jobs=-1)]: Done   4 out of   6 | elapsed:    2.2s remaining:    1.1s
[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.2s remaining:    0.0s
[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.2s finished
并行执行时间 = 2.2567458152770996s
[1, 4, 9, 16, 25, 36]
```



 